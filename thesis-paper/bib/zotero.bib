
@book{kamath_deep_2019,
	address = {Cham},
	title = {Deep {Learning} for {NLP} and {Speech} {Recognition}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-030-14595-8 978-3-030-14596-5},
	url = {http://link.springer.com/10.1007/978-3-030-14596-5},
	language = {en},
	urldate = {2025-03-13},
	publisher = {Springer International Publishing},
	author = {Kamath, Uday and Liu, John and Whitaker, James},
	year = {2019},
	doi = {10.1007/978-3-030-14596-5},
}

@misc{tampuu_multiagent_2015,
	title = {Multiagent {Cooperation} and {Competition} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1511.08779},
	doi = {10.48550/arXiv.1511.08779},
	abstract = {Multiagent systems appear in most social, economical, and political situations. In the present work we extend the Deep Q-Learning Network architecture proposed by Google DeepMind to multiagent environments and investigate how two agents controlled by independent Deep Q-Networks interact in the classic videogame Pong. By manipulating the classical rewarding scheme of Pong we demonstrate how competitive and collaborative behaviors emerge. Competitive agents learn to play and score efﬁciently. Agents trained under collaborative rewarding schemes ﬁnd an optimal strategy to keep the ball in the game as long as possible. We also describe the progression from competitive to collaborative behavior. The present work demonstrates that Deep Q-Networks can become a practical tool for studying the decentralized learning of multiagent systems living in highly complex environments.},
	language = {en},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Tampuu, Ardi and Matiisen, Tambet and Kodelja, Dorian and Kuzovkin, Ilya and Korjus, Kristjan and Aru, Juhan and Aru, Jaan and Vicente, Raul},
	month = nov,
	year = {2015},
	note = {arXiv:1511.08779 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	file = {PDF:/home/linsfa/Zotero/storage/AQJS6N2C/Tampuu et al. - 2015 - Multiagent Cooperation and Competition with Deep Reinforcement Learning.pdf:application/pdf},
}

@article{greff_lstm_2017,
	title = {{LSTM}: {A} {Search} {Space} {Odyssey}},
	volume = {28},
	issn = {2162-237X, 2162-2388},
	shorttitle = {{LSTM}},
	url = {http://arxiv.org/abs/1503.04069},
	doi = {10.1109/TNNLS.2016.2582924},
	abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the ﬁrst large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (≈ 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture signiﬁcantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efﬁcient adjustment.},
	language = {en},
	number = {10},
	urldate = {2025-02-27},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
	month = oct,
	year = {2017},
	note = {arXiv:1503.04069 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	pages = {2222--2232},
	file = {PDF:/home/linsfa/Zotero/storage/DUNYN8HW/Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf:application/pdf},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2025-02-22},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/home/linsfa/Zotero/storage/V2XJJZBL/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@misc{noauthor_httpsarxivorgpdf151203385_nodate,
	title = {https://arxiv.org/pdf/1512.03385},
	url = {https://arxiv.org/pdf/1512.03385},
	urldate = {2025-02-22},
	file = {https\://arxiv.org/pdf/1512.03385:/home/linsfa/Zotero/storage/AAZLPFY6/1512.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	language = {en},
	number = {8},
	urldate = {2025-02-21},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780},
	file = {hochreiter1997:/home/linsfa/Zotero/storage/U2YY5423/hochreiter1997.pdf:application/pdf},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	language = {en},
	author = {Rumelhart, David E and Hintont, Geoffrey E and Williams, Ronald J},
	year = {1986},
	file = {PDF:/home/linsfa/Zotero/storage/2BKH3IQ8/Rumelhart et al. - 1986 - Learning representations by back-propagating errors.pdf:application/pdf},
}

@incollection{calders_learned-norm_2014,
	address = {Berlin, Heidelberg},
	title = {Learned-{Norm} {Pooling} for {Deep} {Feedforward} and {Recurrent} {Neural} {Networks}},
	volume = {8724},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-662-44847-2 978-3-662-44848-9},
	url = {http://link.springer.com/10.1007/978-3-662-44848-9_34},
	abstract = {In this paper we propose and investigate a novel nonlinear unit, called Lp unit, for deep neural networks. The proposed Lp unit receives signals from several projections of a subset of units in the layer below and computes a normalized Lp norm. We notice two interesting interpretations of the Lp unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the Lp unit is, to a certain degree, similar to the recently proposed maxout unit [13] which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the Lp unit is more ecient at representing complex, nonlinear separating boundaries. Each Lp unit denes a superelliptic boundary, with its exact shape dened by the order p. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more eciently by combining a few Lp units of dierent orders. This insight justies the need for learning dierent orders for each unit in the model. We empirically evaluate the proposed Lp units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the Lp units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed Lp unit on the recently proposed deep recurrent neural networks (RNN).},
	language = {en},
	urldate = {2025-02-19},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gulcehre, Caglar and Cho, Kyunghyun and Pascanu, Razvan and Bengio, Yoshua},
	editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
	year = {2014},
	doi = {10.1007/978-3-662-44848-9_34},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {530--546},
	file = {PDF:/home/linsfa/Zotero/storage/S4C2BALB/Gulcehre et al. - 2014 - Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks.pdf:application/pdf},
}

@misc{chung_empirical_2014,
	title = {Empirical {Evaluation} of {Gated} {Recurrent} {Neural} {Networks} on {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1412.3555},
	doi = {10.48550/arXiv.1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	language = {en},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv:1412.3555 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {PDF:/home/linsfa/Zotero/storage/Q2DXB89E/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:application/pdf},
}

@misc{weng_long_2018,
	title = {A ({Long}) {Peek} into {Reinforcement} {Learning}},
	url = {https://lilianweng.github.io/posts/2018-02-19-rl-overview/},
	abstract = {[Updated on 2020-09-03: Updated the algorithm of SARSA and Q-learning so that the difference is more pronounced.

[Updated on 2021-09-19: Thanks to 爱吃猫的鱼, we have this post in Chinese].},
	language = {en},
	urldate = {2025-02-15},
	author = {Weng, Lilian},
	month = feb,
	year = {2018},
	note = {Section: posts},
	file = {Snapshot:/home/linsfa/Zotero/storage/P5ZUWB3L/2018-02-19-rl-overview.html:text/html},
}

@misc{google_deepmind_rl_2015,
	title = {{RL} {Course} by {David} {Silver} - {Lecture} 1: {Introduction} to {Reinforcement} {Learning}},
	shorttitle = {{RL} {Course} by {David} {Silver} - {Lecture} 1},
	url = {https://www.youtube.com/watch?v=2pWv7GOvuf0},
	abstract = {\#Reinforcement Learning Course by David Silver\# Lecture 1:  Introduction to Reinforcement Learning

\#Slides and more info about the course: http://goo.gl/vUiyjq},
	urldate = {2025-02-15},
	author = {{Google DeepMind}},
	month = may,
	year = {2015},
}

@misc{noauthor_understanding_nodate,
	title = {Understanding {LSTM} {Networks} -- colah's blog},
	url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/},
	urldate = {2025-02-15},
	file = {Understanding LSTM Networks -- colah's blog:/home/linsfa/Zotero/storage/CNY98QBY/2015-08-Understanding-LSTMs.html:text/html},
}

@misc{noauthor_unreasonable_nodate,
	title = {The {Unreasonable} {Effectiveness} of {Recurrent} {Neural} {Networks}},
	url = {https://karpathy.github.io/2015/05/21/rnn-effectiveness/},
	urldate = {2025-02-15},
	file = {The Unreasonable Effectiveness of Recurrent Neural Networks:/home/linsfa/Zotero/storage/9HBSY558/rnn-effectiveness.html:text/html},
}

@misc{noauthor_unsupervised_nodate,
	title = {Unsupervised {Feature} {Learning} and {Deep} {Learning} {Tutorial}},
	url = {http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/},
	urldate = {2025-02-15},
	file = {Unsupervised Feature Learning and Deep Learning Tutorial:/home/linsfa/Zotero/storage/EEA2T4MY/MultiLayerNeuralNetworks.html:text/html},
}

@misc{molinghen_laser_2024,
	title = {Laser {Learning} {Environment}: {A} new environment for coordination-critical multi-agent tasks},
	shorttitle = {Laser {Learning} {Environment}},
	url = {http://arxiv.org/abs/2404.03596},
	doi = {10.48550/arXiv.2404.03596},
	abstract = {We introduce the Laser Learning Environment (LLE), a collaborative multi-agent reinforcement learning environment in which coordination is central. In LLE, agents depend on each other to make progress (interdependence), must jointly take specific sequences of actions to succeed (perfect coordination), and accomplishing those joint actions does not yield any intermediate reward (zero-incentive dynamics). The challenge of such problems lies in the difficulty of escaping state space bottlenecks caused by interdependence steps since escaping those bottlenecks is not rewarded. We test multiple state-of-the-art value-based MARL algorithms against LLE and show that they consistently fail at the collaborative task because of their inability to escape state space bottlenecks, even though they successfully achieve perfect coordination. We show that Q-learning extensions such as prioritised experience replay and n-steps return hinder exploration in environments with zero-incentive dynamics, and find that intrinsic curiosity with random network distillation is not sufficient to escape those bottlenecks. We demonstrate the need for novel methods to solve this problem and the relevance of LLE as cooperative MARL benchmark.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Molinghen, Yannick and Avalos, Raphaël and Achter, Mark Van and Nowé, Ann and Lenaerts, Tom},
	month = apr,
	year = {2024},
	note = {arXiv:2404.03596 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {PDF:/home/linsfa/Zotero/storage/5Y3UBCNL/Molinghen et al. - 2024 - Laser Learning Environment A new environment for coordination-critical multi-agent tasks.pdf:application/pdf},
}

@misc{samvelyan_starcraft_2019,
	title = {The {StarCraft} {Multi}-{Agent} {Challenge}},
	url = {http://arxiv.org/abs/1902.04043},
	doi = {10.48550/arXiv.1902.04043},
	abstract = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Samvelyan, Mikayel and Rashid, Tabish and Witt, Christian Schroeder de and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G. J. and Hung, Chia-Man and Torr, Philip H. S. and Foerster, Jakob and Whiteson, Shimon},
	month = dec,
	year = {2019},
	note = {arXiv:1902.04043 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
	file = {PDF:/home/linsfa/Zotero/storage/S6UCLAIG/Samvelyan et al. - 2019 - The StarCraft Multi-Agent Challenge.pdf:application/pdf},
}

@misc{wang_qplex_2021,
	title = {{QPLEX}: {Duplex} {Dueling} {Multi}-{Agent} {Q}-{Learning}},
	shorttitle = {{QPLEX}},
	url = {http://arxiv.org/abs/2008.01062},
	doi = {10.48550/arXiv.2008.01062},
	abstract = {We explore value-based multi-agent reinforcement learning (MARL) in the popular paradigm of centralized training with decentralized execution (CTDE). CTDE has an important concept, Individual-Global-Max (IGM) principle, which requires the consistency between joint and local action selections to support efﬁcient local decision-making. However, in order to achieve scalability, existing MARL methods either limit representation expressiveness of their value function classes or relax the IGM consistency, which may suffer from instability risk or may not perform well in complex domains. This paper presents a novel MARL approach, called duPLEX dueling multi-agent Q-learning (QPLEX), which takes a duplex dueling network architecture to factorize the joint value function. This duplex dueling structure encodes the IGM principle into the neural network architecture and thus enables efﬁcient value function learning. Theoretical analysis shows that QPLEX achieves a complete IGM function class. Empirical experiments on StarCraft II micromanagement tasks demonstrate that QPLEX signiﬁcantly outperforms stateof-the-art baselines in both online and ofﬂine data collection settings, and also reveal that QPLEX achieves high sample efﬁciency and can beneﬁt from ofﬂine datasets without additional online exploration1.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Wang, Jianhao and Ren, Zhizhou and Liu, Terry and Yu, Yang and Zhang, Chongjie},
	month = oct,
	year = {2021},
	note = {arXiv:2008.01062 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
	file = {PDF:/home/linsfa/Zotero/storage/DR4X7VW2/Wang et al. - 2021 - QPLEX Duplex Dueling Multi-Agent Q-Learning.pdf:application/pdf},
}

@misc{rashid_qmix_2018,
	title = {{QMIX}: {Monotonic} {Value} {Function} {Factorisation} for {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	shorttitle = {{QMIX}},
	url = {http://arxiv.org/abs/1803.11485},
	doi = {10.48550/arXiv.1803.11485},
	abstract = {In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint actionvalues conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX signiﬁcantly outperforms existing value-based multi-agent reinforcement learning methods.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Rashid, Tabish and Samvelyan, Mikayel and Witt, Christian Schroeder de and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
	month = jun,
	year = {2018},
	note = {arXiv:1803.11485 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
	file = {PDF:/home/linsfa/Zotero/storage/PD2VYDIV/Rashid et al. - 2018 - QMIX Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.pdf:application/pdf},
}

@misc{sunehag_value-decomposition_2017,
	title = {Value-{Decomposition} {Networks} {For} {Cooperative} {Multi}-{Agent} {Learning}},
	url = {http://arxiv.org/abs/1706.05296},
	doi = {10.48550/arXiv.1706.05296},
	abstract = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difﬁcult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we ﬁnd the problem of spurious rewards and a phenomenon we call the “lazy agent” problem, which arises due to partial observability. We address these problems by training individual agents with a novel value decomposition network architecture, which learns to decompose the team value function into agent-wise value functions. We perform an experimental evaluation across a range of partially-observable multi-agent domains and show that learning such value-decompositions leads to superior results, in particular when combined with weight sharing, role information and information channels.},
	language = {en},
	urldate = {2025-01-31},
	publisher = {arXiv},
	author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
	month = jun,
	year = {2017},
	note = {arXiv:1706.05296 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {PDF:/home/linsfa/Zotero/storage/HGGCEGPS/Sunehag et al. - 2017 - Value-Decomposition Networks For Cooperative Multi-Agent Learning.pdf:application/pdf},
}

@article{cao_overview_2013,
	title = {An {Overview} of {Recent} {Progress} in the {Study} of {Distributed} {Multi}-{Agent} {Coordination}},
	volume = {9},
	issn = {1941-0050},
	url = {https://ieeexplore.ieee.org/document/6303906},
	doi = {10.1109/TII.2012.2219061},
	abstract = {This paper reviews some main results and progress in distributed multi-agent coordination, focusing on papers published in major control systems and robotics journals since 2006. Distributed coordination of multiple vehicles, including unmanned aerial vehicles, unmanned ground vehicles, and unmanned underwater vehicles, has been a very active research subject studied extensively by the systems and control community. The recent results in this area are categorized into several directions, such as consensus, formation control, optimization, and estimation. After the review, a short discussion section is included to summarize the existing research and to propose several promising research directions along with some open problems that are deemed important for further investigations.},
	number = {1},
	urldate = {2025-03-13},
	journal = {IEEE Transactions on Industrial Informatics},
	author = {Cao, Yongcan and Yu, Wenwu and Ren, Wei and Chen, Guanrong},
	month = feb,
	year = {2013},
	note = {Conference Name: IEEE Transactions on Industrial Informatics},
	keywords = {Algorithm design and analysis, Delay, Delay effects, Distributed coordination, formation control, Heuristic algorithms, multi-agent system, Network topology, sensor network, Vehicle dynamics, Vehicles},
	pages = {427--438},
	file = {IEEE Xplore Abstract Record:/home/linsfa/Zotero/storage/U9JBUNWH/6303906.html:text/html;Submitted Version:/home/linsfa/Zotero/storage/SJRFRZP2/Cao et al. - 2013 - An Overview of Recent Progress in the Study of Distributed Multi-Agent Coordination.pdf:application/pdf},
}

@article{klima_space_2018,
	title = {Space {Debris} {Removal}: {Learning} to {Cooperate} and the {Price} of {Anarchy}},
	volume = {5},
	issn = {2296-9144},
	shorttitle = {Space {Debris} {Removal}},
	url = {https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2018.00054/full},
	doi = {10.3389/frobt.2018.00054},
	abstract = {{\textless}p{\textgreater}In this paper we study space debris removal from a game-theoretic perspective. In particular we focus on the question whether and how self-interested agents can cooperate in this dilemma, which resembles a tragedy of the commons scenario. We compare centralised and decentralised solutions and the corresponding price of anarchy, which measures the extent to which competition approximates cooperation. In addition we investigate whether agents can learn optimal strategies by reinforcement learning. To this end, we improve on an existing high fidelity orbital simulator, and use this simulator to obtain a computationally efficient surrogate model that can be used for our subsequent game-theoretic analysis. We study both single- and multi-agent approaches using stochastic (Markov) games and reinforcement learning. The main finding is that the cost of a decentralised, competitive solution can be significant, which should be taken into consideration when forming debris removal strategies.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-03-13},
	journal = {Frontiers in Robotics and AI},
	author = {Klima, Richard and Bloembergen, Daan and Savani, Rahul and Tuyls, Karl and Wittig, Alexander and Sapera, Andrei and Izzo, Dario},
	month = jun,
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {Active debris removal, Markov decision process, Price of anarchy, space debris, tragedy of the commons},
	file = {Full Text PDF:/home/linsfa/Zotero/storage/FAZFYTKD/Klima et al. - 2018 - Space Debris Removal Learning to Cooperate and the Price of Anarchy.pdf:application/pdf},
}

@misc{minelli_comix_2024,
	title = {{CoMIX}: {A} {Multi}-agent {Reinforcement} {Learning} {Training} {Architecture} for {Efficient} {Decentralized} {Coordination} and {Independent} {Decision}-{Making}},
	shorttitle = {{CoMIX}},
	url = {http://arxiv.org/abs/2308.10721},
	doi = {10.48550/arXiv.2308.10721},
	abstract = {Robust coordination skills enable agents to operate cohesively in shared environments, together towards a common goal and, ideally, individually without hindering each other’s progress. To this end, this paper presents Coordinated QMIX (CoMIX), a novel training framework for decentralized agents that enables emergent coordination through flexible policies, allowing at the same time independent decision-making at individual level. CoMIX models selfish and collaborative behavior as incremental steps in each agent’s decision process. This allows agents to dynamically adapt their behavior to different situations balancing independence and collaboration. Experiments using a variety of simulation environments demonstrate that CoMIX outperforms baselines on collaborative tasks. The results validate our incremental approach as effective technique for improving coordination in multi-agent systems.},
	language = {en},
	urldate = {2025-03-30},
	publisher = {arXiv},
	author = {Minelli, Giovanni and Musolesi, Mirco},
	month = dec,
	year = {2024},
	note = {arXiv:2308.10721 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {PDF:/home/linsfa/Zotero/storage/CCXK7I8V/Minelli and Musolesi - 2024 - CoMIX A Multi-agent Reinforcement Learning Training Architecture for Efficient Decentralized Coordi.pdf:application/pdf},
}

@misc{rashid_weighted_2020,
	title = {Weighted {QMIX}: {Expanding} {Monotonic} {Value} {Function} {Factorisation} for {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	shorttitle = {Weighted {QMIX}},
	url = {http://arxiv.org/abs/2006.10800},
	doi = {10.48550/arXiv.2006.10800},
	abstract = {QMIX is a popular \$Q\$-learning algorithm for cooperative MARL in the centralised training and decentralised execution paradigm. In order to enable easy decentralisation, QMIX restricts the joint action \$Q\$-values it can represent to be a monotonic mixing of each agent's utilities. However, this restriction prevents it from representing value functions in which an agent's ordering over its actions can depend on other agents' actions. To analyse this representational limitation, we first formalise the objective QMIX optimises, which allows us to view QMIX as an operator that first computes the \$Q\$-learning targets and then projects them into the space representable by QMIX. This projection returns a representable \$Q\$-value that minimises the unweighted squared error across all joint actions. We show in particular that this projection can fail to recover the optimal policy even with access to \$Q{\textasciicircum}*\$, which primarily stems from the equal weighting placed on each joint action. We rectify this by introducing a weighting into the projection, in order to place more importance on the better joint actions. We propose two weighting schemes and prove that they recover the correct maximal action for any joint action \$Q\$-values, and therefore for \$Q{\textasciicircum}*\$ as well. Based on our analysis and results in the tabular setting, we introduce two scalable versions of our algorithm, Centrally-Weighted (CW) QMIX and Optimistically-Weighted (OW) QMIX and demonstrate improved performance on both predator-prey and challenging multi-agent StarCraft benchmark tasks.},
	urldate = {2025-03-30},
	publisher = {arXiv},
	author = {Rashid, Tabish and Farquhar, Gregory and Peng, Bei and Whiteson, Shimon},
	month = oct,
	year = {2020},
	note = {arXiv:2006.10800 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
	file = {Preprint PDF:/home/linsfa/Zotero/storage/BZ9UCGNY/Rashid et al. - 2020 - Weighted QMIX Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement L.pdf:application/pdf;Snapshot:/home/linsfa/Zotero/storage/WZIIVENW/2006.html:text/html},
}

@misc{dmap_2020_icaps_factored_2020,
	title = {Factored {Value} {Functions} for {Cooperative} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {https://www.youtube.com/watch?v=EsCwDYtBZ8M},
	urldate = {2025-03-30},
	author = {{DMAP 2020 ICAPS}},
	month = nov,
	year = {2020},
}

@article{laurent_world_2011,
	title = {The world of independent learners is not markovian},
	volume = {15},
	issn = {18758827, 13272314},
	url = {https://journals.sagepub.com/doi/full/10.3233/KES-2010-0206},
	doi = {10.3233/KES-2010-0206},
	abstract = {In multi-agent systems, the presence of learning agents can cause the environment to be non-Markovian from an agent’s perspective thus violating the property that traditional single-agent learning methods rely upon. This paper formalizes some known intuition about concurrently learning agents by providing formal conditions that make the environment non-Markovian from an independent (non-communicative) learner’s perspective. New concepts are introduced like the divergent learning paths and the observability of the effects of others’ actions. To illustrate the formal concepts, a case study is also presented. These ﬁndings are signiﬁcant because they both help to understand failures and successes of existing learning algorithms as well as being suggestive for future work.},
	language = {en},
	number = {1},
	urldate = {2025-03-18},
	journal = {International Journal of Knowledge-based and Intelligent Engineering Systems},
	author = {Laurent, Guillaume J. and Matignon, Laëtitia and Le Fort-Piat, N.},
	month = mar,
	year = {2011},
	pages = {55--64},
	file = {PDF:/home/linsfa/Zotero/storage/4CAE43RU/Laurent et al. - 2011 - The world of independent learners is not markovian.pdf:application/pdf},
}

@misc{noauthor_agent_nodate,
	title = {Agent {Laboratory}: {Using} {LLMs} as {Research} {Assistants}},
	shorttitle = {Agent {Laboratory}},
	url = {https://agentlaboratory.github.io/},
	abstract = {by Samuel Schmidgall at JHU},
	urldate = {2025-03-15},
	file = {PDF:/home/linsfa/Zotero/storage/79X5CD6T/Agent Laboratory Using LLMs as Research Assistants.pdf:application/pdf;Snapshot:/home/linsfa/Zotero/storage/X456I38X/agentlaboratory.github.io.html:text/html},
}

@book{sutton_reinforcement_2014,
	address = {Cambridge, Massachusetts},
	edition = {Nachdruck},
	series = {Adaptive computation and machine learning},
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	shorttitle = {Reinforcement learning},
	language = {en},
	publisher = {The MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew},
	year = {2014},
	file = {PDF:/home/linsfa/Zotero/storage/RIAFC9CW/Sutton and Barto - 2014 - Reinforcement learning an introduction.pdf:application/pdf},
}

@article{panait_cooperative_2005,
	title = {Cooperative {Multi}-{Agent} {Learning}: {The} {State} of the {Art}},
	volume = {11},
	copyright = {http://www.springer.com/tdm},
	issn = {1387-2532, 1573-7454},
	shorttitle = {Cooperative {Multi}-{Agent} {Learning}},
	url = {http://link.springer.com/10.1007/s10458-005-2631-2},
	doi = {10.1007/s10458-005-2631-2},
	abstract = {Cooperative multi-agent systems (MAS) are ones in which several agents attempt, through their interaction, to jointly solve tasks or to maximize utility. Due to the interactions among the agents, multi-agent problem complexity can rise rapidly with the number of agents or their behavioral sophistication. The challenge this presents to the task of programming solutions to MAS problems has spawned increasing interest in machine learning techniques to automate the search and optimization process. We provide a broad survey of the cooperative multi-agent learning literature. Previous surveys of this area have largely focused on issues common to speciﬁc subareas (for example, reinforcement learning, RL or robotics). In this survey we attempt to draw from multi-agent learning work in a spectrum of areas, including RL, evolutionary computation, game theory, complex systems, agent modeling, and robotics. We ﬁnd that this broad view leads to a division of the work into two categories, each with its own special issues: applying a single learner to discover joint solutions to multi-agent problems (team learning), or using multiple simultaneous learners, often one per agent (concurrent learning). Additionally, we discuss direct and indirect communication in connection with learning, plus open issues in task decomposition, scalability, and adaptive dynamics. We conclude with a presentation of multi-agent learning problem domains, and a list of multi-agent learning resources.},
	language = {en},
	number = {3},
	urldate = {2025-03-15},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Panait, Liviu and Luke, Sean},
	month = nov,
	year = {2005},
	pages = {387--434},
	file = {Cooperative Multi-Agent Learning_ State of the Art (2005):/home/linsfa/Zotero/storage/MQUFD42Z/Cooperative Multi-Agent Learning_ State of the Art (2005).wav:audio/x-wav;PDF:/home/linsfa/Zotero/storage/IT56A7SS/Panait and Luke - 2005 - Cooperative Multi-Agent Learning The State of the Art.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-04-07},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/linsfa/Zotero/storage/EEKZ22S4/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:/home/linsfa/Zotero/storage/MKG454NT/1706.html:text/html},
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/settings/storage?ref=usb},
	urldate = {2025-04-18},
	file = {Zotero | Your personal research assistant:/home/linsfa/Zotero/storage/565F66CY/storage.html:text/html},
}

@article{oliehoek_optimal_2008,
	title = {Optimal and {Approximate} {Q}-value {Functions} for {Decentralized} {POMDPs}},
	volume = {32},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1111.0062},
	doi = {10.1613/jair.2447},
	abstract = {Decision-theoretic planning is a popular approach to sequential decision making problems, because it treats uncertainty in sensing and acting in a principled way. In single-agent frameworks like MDPs and POMDPs, planning can be carried out by resorting to Q-value functions: an optimal Q-value function Q* is computed in a recursive manner by dynamic programming, and then an optimal policy is extracted from Q*. In this paper we study whether similar Q-value functions can be defined for decentralized POMDP models (Dec-POMDPs), and how policies can be extracted from such value functions. We define two forms of the optimal Q-value function for Dec-POMDPs: one that gives a normative description as the Q-value function of an optimal pure joint policy and another one that is sequentially rational and thus gives a recipe for computation. This computation, however, is infeasible for all but the smallest problems. Therefore, we analyze various approximate Q-value functions that allow for efficient computation. We describe how they relate, and we prove that they all provide an upper bound to the optimal Q-value function Q*. Finally, unifying some previous approaches for solving Dec-POMDPs, we describe a family of algorithms for extracting policies from such Q-value functions, and perform an experimental evaluation on existing test problems, including a new firefighting benchmark problem.},
	urldate = {2025-04-18},
	journal = {Journal of Artificial Intelligence Research},
	author = {Oliehoek, Frans A. and Spaan, Matthijs T. J. and Vlassis, Nikos},
	month = may,
	year = {2008},
	note = {arXiv:1111.0062 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {289--353},
	file = {Preprint PDF:/home/linsfa/Zotero/storage/QYSIGG5P/Oliehoek et al. - 2008 - Optimal and Approximate Q-value Functions for Decentralized POMDPs.pdf:application/pdf;Snapshot:/home/linsfa/Zotero/storage/REUK2LLT/1111.html:text/html},
}

@article{busoniu_comprehensive_2008,
	title = {A {Comprehensive} {Survey} of {Multiagent} {Reinforcement} {Learning}},
	url = {https://repository.tudelft.nl/record/uuid:4c7d3b49-06fc-400c-923e-3903b8d230fe},
	abstract = {Multiagent systems are rapidly finding applications in a variety of domains, including robotics, distributed control, telecommunications, and economics. The complexity ofmany tasks arising in these domains makes them difficult to solve with preprogrammed agent behaviors. The agents must, instead, discover a solution on their own, using learning. A significant part of the research on multiagent learning concerns reinforcement learning techniques. This paper provides a comprehensive survey of multiagent reinforcement learning (MARL). A central issue in the field is the formal statement of the multiagent learning goal. Different viewpoints on this issue have led to the proposal of many different goals, among which two focal points can be distinguished: stability of the agents’ learning dynamics, and adaptation to the changing behavior of the other agents. The MARL algorithms described in the literature aim—either explicitly or implicitly—at one of these two goals or at a combination of both, in a fully cooperative, fully competitive, or more general setting. A representative selection of these algorithms is discussed in detail in this paper, together with the specific issues that arise in each category. Additionally, the benefits and challenges ofMARL are described along with some of the problem domains where the MARL techniques have been applied. Finally, an outlook for the field is provided.},
	language = {en},
	urldate = {2025-04-18},
	author = {Busoniu, L. and Babuska, R. and De Schutter, B.},
	year = {2008},
	file = {Full Text PDF:/home/linsfa/Zotero/storage/DJMFSHJL/Busoniu et al. - 2008 - A Comprehensive Survey of Multiagent Reinforcement Learning.pdf:application/pdf},
}

@misc{noauthor_httpsjairorgindexphpjairarticledownload1095226090_nodate,
	title = {https://jair.org/index.php/jair/article/download/10952/26090},
	url = {https://jair.org/index.php/jair/article/download/10952/26090},
	urldate = {2025-04-21},
}

@article{bloembergen_evolutionary_2015,
	title = {Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}: {A} {Survey}},
	volume = {53},
	issn = {1076-9757},
	shorttitle = {Evolutionary {Dynamics} of {Multi}-{Agent} {Learning}},
	url = {https://jair.org/index.php/jair/article/view/10952},
	doi = {10.1613/jair.4818},
	abstract = {The interaction of multiple autonomous agents gives rise to highly dynamic and nondeterministic environments, contributing to the complexity in applications such as automated ﬁnancial markets, smart grids, or robotics. Due to the sheer number of situations that may arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents can learn their optimal behaviour and adapt to new situations or circumstances. The past two decades have seen the emergence of reinforcement learning, both in single and multiagent settings, as a strong, robust and adaptive learning paradigm. Progress has been substantial, and a wide range of algorithms are now available. An important challenge in the domain of multi-agent learning is to gain qualitative insights into the resulting system dynamics. In the past decade, tools and methods from evolutionary game theory have been successfully employed to study multi-agent learning dynamics formally in strategic interactions. This article surveys the dynamical models that have been derived for various multi-agent reinforcement learning algorithms, making it possible to study and compare them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be used to study complex strategic interactions. Examples of such analysis are given for the domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing the evolutionary dynamics of multi-agent learning by highlighting the main results and accomplishments.},
	language = {en},
	urldate = {2025-04-21},
	journal = {Journal of Artificial Intelligence Research},
	author = {Bloembergen, Daan and Tuyls, Karl and Hennes, Daniel and Kaisers, Michael},
	month = aug,
	year = {2015},
	pages = {659--697},
	file = {PDF:/home/linsfa/Zotero/storage/DZ5W7CD8/Bloembergen et al. - 2015 - Evolutionary Dynamics of Multi-Agent Learning A Survey.pdf:application/pdf},
}

@inproceedings{elango_diesel_2018,
	address = {Philadelphia PA USA},
	title = {Diesel: {DSL} for linear algebra and neural net computations on {GPUs}},
	isbn = {978-1-4503-5834-7},
	shorttitle = {Diesel},
	url = {https://dl.acm.org/doi/10.1145/3211346.3211354},
	doi = {10.1145/3211346.3211354},
	abstract = {We present a domain specific language compiler, Diesel, for basic linear algebra and neural network computations, that accepts input expressions in an intuitive form and generates high performing code for GPUs. The current trend is to represent a neural network as a computation DAG, where each node in the DAG corresponds to a single operation such as matrix-matrix multiplication, and map the individual operations to hand tuned library functions provided by standard libraries such as CuBLAS and CuDNN. While this method takes advantage of readily available optimized library codes to achieve good performance for individual operations, it is not possible to optimize across operations. As opposed to this, given a computation composed of several operations, Diesel generates (a set) of efficient device functions, where the code is optimized for the computation as a whole, using polyhedral compilation techniques. In addition, there are cases where the code needs to be specialized for specific problem sizes to achieve optimal performance. While standard libraries are written for parametric problem sizes (where problem sizes are provided at runtime), Diesel can accept problem sizes at compile time and generate specialized codes. Experimental results show that the performance achieved by Diesel generated code for individual operations are comparable to the highly tuned versions provided by standard libraries, while for composite computations, Diesel outperforms manually written versions.},
	language = {en},
	urldate = {2025-05-05},
	booktitle = {Proceedings of the 2nd {ACM} {SIGPLAN} {International} {Workshop} on {Machine} {Learning} and {Programming} {Languages}},
	publisher = {ACM},
	author = {Elango, Venmugil and Rubin, Norm and Ravishankar, Mahesh and Sandanagobalane, Hariharan and Grover, Vinod},
	month = jun,
	year = {2018},
	pages = {42--51},
	file = {PDF:/home/linsfa/Zotero/storage/GDV98FGF/Elango et al. - 2018 - Diesel DSL for linear algebra and neural net computations on GPUs.pdf:application/pdf},
}

@misc{singh_constraintflow_2024,
	title = {{ConstraintFlow}: {A} {DSL} for {Specification} and {Verification} of {Neural} {Network} {Analyses}},
	shorttitle = {{ConstraintFlow}},
	url = {http://arxiv.org/abs/2403.18729},
	doi = {10.48550/arXiv.2403.18729},
	abstract = {We develop a declarative DSL - ConstraintFlow- that can be used to specify Abstract Interpretation-based DNN certiﬁers. In ConstraintFlow, programmers can easily deﬁne various existing and new abstract domains and transformers, all within just a few 10s of Lines of Code as opposed to 1000s of LOCs of existing libraries. We provide lightweight automatic veriﬁcation, which can be used to ensure the over-approximation-based soundness of the certiﬁer code written in ConstraintFlow for arbitrary (but bounded) DNN architectures. Using this automated veriﬁcation procedure, for the ﬁrst time, we can verify the soundness of state-of-the-art DNN certiﬁers for arbitrary DNN architectures, all within a few minutes.},
	language = {en},
	urldate = {2025-05-05},
	publisher = {arXiv},
	author = {Singh, Avaljot and Sarita, Yasmin and Mendis, Charith and Singh, Gagandeep},
	month = oct,
	year = {2024},
	note = {arXiv:2403.18729 [cs]},
	keywords = {Computer Science - Programming Languages},
	file = {PDF:/home/linsfa/Zotero/storage/P6XDHCZS/Singh et al. - 2024 - ConstraintFlow A DSL for Specification and Verification of Neural Network Analyses.pdf:application/pdf},
}

@article{bellman_markovian_1957,
	title = {A {Markovian} {Decision} {Process}},
	volume = {6},
	issn = {0095-9057},
	url = {https://www.jstor.org/stable/24900506},
	number = {5},
	urldate = {2025-04-25},
	journal = {Journal of Mathematics and Mechanics},
	author = {Bellman, Richard},
	year = {1957},
	note = {Publisher: Indiana University Mathematics Department},
	pages = {679--684},
	file = {[Indiana University Mathematics Journal vol. 6 iss. 4] Bellman, Richard - A Markovian Decision Process (1957) [10.1512_iumj.1957.6.56038] - libgen.li:/home/linsfa/Zotero/storage/SUYWZJTE/[Indiana University Mathematics Journal vol. 6 iss. 4] Bellman, Richard - A Markovian Decision Process (1957) [10.1512_iumj.1957.6.56038] - libgen.li.pdf:application/pdf},
}

@article{bard_hanabi_2020,
	title = {The {Hanabi} challenge: {A} new frontier for {AI} research},
	volume = {280},
	issn = {0004-3702},
	shorttitle = {The {Hanabi} challenge},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370219300116},
	doi = {10.1016/j.artint.2019.103216},
	abstract = {From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.},
	urldate = {2025-07-29},
	journal = {Artificial Intelligence},
	author = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},
	month = mar,
	year = {2020},
	keywords = {Challenge paper, Communication, Cooperative, Games, Imperfect information, Multi-agent learning, Reinforcement learning, Theory of mind},
	pages = {103216},
	file = {ScienceDirect Full Text PDF:/home/linsfa/Zotero/storage/C4H99934/Bard et al. - 2020 - The Hanabi challenge A new frontier for AI research.pdf:application/pdf;ScienceDirect Snapshot:/home/linsfa/Zotero/storage/9RWCXBVJ/S0004370219300116.html:text/html},
}

@book{weiss_multiagent_2001,
	address = {Cambridge, Mass.},
	edition = {3. print},
	title = {Multiagent systems: a modern approach to distributed artificial intelligence},
	isbn = {978-0-262-23203-6 978-0-262-73131-7},
	shorttitle = {Multiagent systems},
	language = {en},
	publisher = {MIT Press},
	editor = {Weiss, Gerhard},
	year = {2001},
	file = {PDF:/home/linsfa/Zotero/storage/I7SS3XZA/Weiss - 2001 - Multiagent systems a modern approach to distributed artificial intelligence.pdf:application/pdf},
}

@techreport{stone_multiagent_1997,
	address = {Fort Belvoir, VA},
	title = {Multiagent {Systems}: {A} {Survey} from a {Machine} {Learning} {Perspective}:},
	shorttitle = {Multiagent {Systems}},
	url = {http://www.dtic.mil/docs/citations/ADA333248},
	abstract = {Distributed Artiﬁcial Intelligence (DAI) has existed as a subﬁeld of AI for less than two decades. DAI is concerned with systems that consist of multiple independent entities that interact in a domain. Traditionally, DAI has been divided into two sub-disciplines: Distributed Problem Solving (DPS) focuses on the information management aspects of systems with several components working together towards a common goal; Multiagent Systems (MAS) deals with behavior management in collections of several independent entities, or agents. This survey of MAS is intended to serve as an introduction to the ﬁeld and as an organizational framework. A series of general multiagent scenarios are presented. For each scenario, the issues that arise are described along with a sampling of the techniques that exist to deal with them. The presented techniques are not exhaustive, but they highlight how multiagent systems can be and have been used to build complex systems. When options exist, the techniques presented are biased towards machine learning approaches. Additional opportunities for applying machine learning to MAS are highlighted and robotic soccer is presented as an appropriate test bed for MAS. This survey does not focus exclusively on robotic systems. However, we believe that much of the prior research in non-robotic MAS is relevant to robotic MAS, and we explicitly discuss several robotic MAS, including all of those presented in this issue.},
	language = {en},
	urldate = {2025-07-29},
	institution = {Defense Technical Information Center},
	author = {Stone, Peter and Veloso, Manuela},
	month = dec,
	year = {1997},
	doi = {10.21236/ADA333248},
	file = {PDF:/home/linsfa/Zotero/storage/52SYBHD8/Stone and Veloso - 1997 - Multiagent Systems A Survey from a Machine Learning Perspective.pdf:application/pdf},
}

@article{boutilier_planning_nodate,
	title = {Planning, {Learning} and {Coordination} in {Multiagent} {Decision} {Processes}},
	language = {en},
	author = {Boutilier, Craig},
	file = {PDF:/home/linsfa/Zotero/storage/IXE7I9WS/Boutilier - Planning, Learning and Coordination in Multiagent Decision Processes.pdf:application/pdf},
}

@book{puterman_markov_2009,
	address = {Hoboken},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Markov {Decision} {Processes}: {Discrete} {Stochastic} {Dynamic} {Programming}},
	isbn = {978-0-471-72782-8},
	shorttitle = {Markov {Decision} {Processes}},
	abstract = {The Wiley-Interscience Paperback Series consists of selected books that have been made more accessible to consumers in an effort to increase global appeal and general circulation. With these new unabridged softcover volumes, Wiley hopes to extend the lives of these works by making them available to future generations of statisticians, mathematicians, and scientists."This text is unique in bringing together so many results hitherto found only in part in other texts and papers. . . . The text is fairly self-contained, inclusive of some basic mathematical results needed, and provides a rich diet},
	language = {en},
	number = {v.414},
	publisher = {John Wiley \& Sons, Inc},
	author = {Puterman, Martin L.},
	year = {2009},
	file = {PDF:/home/linsfa/Zotero/storage/G56M4F2W/Puterman - 2009 - Markov Decision Processes Discrete Stochastic Dynamic Programming.pdf:application/pdf},
}

@article{bowling_analysis_nodate,
	title = {An {Analysis} of {Stochastic} {Game} {Theory} for {Multiagent} {Reinforcement} {Learning}},
	abstract = {Learning behaviors in a multiagent environment is crucial for developing and adapting multiagent systems. Reinforcement learning techniques have addressed this problem for a single agent acting in a stationary environment, which is modeled as a Markov decision process (MDP). But, multiagent environments are inherently non-stationary since the other agents are free to change their behavior as they also learn and adapt. Stochastic games, ﬁrst studied in the game theory community, are a natural extension of MDPs to include multiple agents. In this paper we contribute a comprehensive presentation of the relevant techniques for solving stochastic games from both the game theory community and reinforcement learning communities. We examine the assumptions and limitations of these algorithms, and identify similarities between these algorithms, single agent reinforcement learners, and basic game theory techniques.},
	language = {en},
	author = {Bowling, Michael and Veloso, Manuela},
	file = {PDF:/home/linsfa/Zotero/storage/PF2GV5QA/Bowling and Veloso - An Analysis of Stochastic Game Theory for Multiagent Reinforcement Learning.pdf:application/pdf},
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	language = {en},
	number = {7540},
	urldate = {2025-08-01},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	pages = {529--533},
	file = {PDF:/home/linsfa/Zotero/storage/G64UURZV/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:application/pdf},
}

@misc{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	doi = {10.48550/arXiv.1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their signiﬁcance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efﬁciently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new stateof-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	language = {en},
	urldate = {2025-08-04},
	publisher = {arXiv},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = feb,
	year = {2016},
	note = {arXiv:1511.05952 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/home/linsfa/Zotero/storage/NITUWUJR/Schaul et al. - 2016 - Prioritized Experience Replay.pdf:application/pdf},
}

@article{works_computing_1950,
	title = {Computing {Machinery} and {Intelligence}},
	volume = {59},
	url = {http://www.jstor.org/stable/2251299},
	language = {en},
	number = {236},
	journal = {Mind, New Series},
	author = {work(s):, A. M. Turing Reviewed},
	year = {1950},
	pages = {433--460},
	file = {PDF:/home/linsfa/Zotero/storage/4UTWZMPL/work(s) - 1950 - Computing Machinery and Intelligence.pdf:application/pdf},
}
